{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a2f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['HADOOP_HOME'] = \"C:\\\\hadoop\"\n",
    "# os.environ['hadoop.home.dir'] = \"C:\\\\hadoop\"\n",
    "# os.environ['PATH'] += os.pathsep + \"C:\\\\hadoop\\\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529ab8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e827cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+---+----+-------+-----+\n",
      "|class|name|kor|eng|math|science|total|\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "|    1| aaa| 67| 87|  90|     98|  342|\n",
      "|    1| bbb| 45| 45|  56|     98|  244|\n",
      "|    1| ccc| 95| 59|  96|     88|  338|\n",
      "|    1| ddd| 65| 94|  89|     98|  346|\n",
      "|    1| eee| 45| 65|  78|     98|  286|\n",
      "|    1| fff| 78| 76|  98|     89|  341|\n",
      "|    2| ggg| 87| 67|  65|     56|  275|\n",
      "|    2| hhh| 89| 98|  78|     78|  343|\n",
      "|    2| iii|100| 78|  56|     65|  299|\n",
      "|    2| jjj| 99| 89|  87|     87|  362|\n",
      "|    2| kkk| 98| 45|  56|     54|  253|\n",
      "|    2| lll| 65| 89|  87|     78|  319|\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Save Example\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"data/Subjects.csv\", header=True, inferSchema=True)\n",
    "df = df.withColumn(\"total\", col(\"kor\") + col(\"eng\") + col(\"math\") + col(\"science\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9cca61-f3f5-4bdc-8627-c6db780f760d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---+---+----+-------+-----+\n",
      "|class|name|kor|eng|math|science|total|\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "|    1| aaa| 67| 87|  90|     98|  342|\n",
      "|    1| bbb| 45| 45|  56|     98|  244|\n",
      "|    1| ccc| 95| 59|  96|     88|  338|\n",
      "|    1| ddd| 65| 94|  89|     98|  346|\n",
      "|    1| eee| 45| 65|  78|     98|  286|\n",
      "|    1| fff| 78| 76|  98|     89|  341|\n",
      "|    2| ggg| 87| 67|  65|     56|  275|\n",
      "|    2| hhh| 89| 98|  78|     78|  343|\n",
      "|    2| iii|100| 78|  56|     65|  299|\n",
      "|    2| jjj| 99| 89|  87|     87|  362|\n",
      "|    2| kkk| 98| 45|  56|     54|  253|\n",
      "|    2| lll| 65| 89|  87|     78|  319|\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "\n",
      "+-----------------+\n",
      "|       avg(total)|\n",
      "+-----------------+\n",
      "|312.3333333333333|\n",
      "+-----------------+\n",
      "\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "|class|name|kor|eng|math|science|total|\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "|    1| aaa| 67| 87|  90|     98|  342|\n",
      "|    1| ccc| 95| 59|  96|     88|  338|\n",
      "|    1| ddd| 65| 94|  89|     98|  346|\n",
      "|    1| fff| 78| 76|  98|     89|  341|\n",
      "|    2| hhh| 89| 98|  78|     78|  343|\n",
      "|    2| jjj| 99| 89|  87|     87|  362|\n",
      "|    2| lll| 65| 89|  87|     78|  319|\n",
      "+-----+----+---+---+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 컬럼 추가: 총점\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"total\", col(\"kor\") + col(\"eng\") + col(\"math\") + col(\"science\"))\n",
    "df.show()\n",
    "\n",
    "# 평균점수 계산\n",
    "df.groupBy().avg(\"total\").show()\n",
    "\n",
    "# 조건 필터링: 총점 300 이상\n",
    "df.filter(col(\"total\") >= 300).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b1ab28d-98e2-4bf0-b7dc-9c2788cbeb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|total|\n",
      "+----+-----+\n",
      "| aaa|  342|\n",
      "| ccc|  338|\n",
      "| ddd|  346|\n",
      "| fff|  341|\n",
      "| hhh|  343|\n",
      "| jjj|  362|\n",
      "| lll|  319|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Temp View 생성\n",
    "df.createOrReplaceTempView(\"students\")\n",
    "\n",
    "# SQL 쿼리\n",
    "high_score = spark.sql(\"SELECT name, total FROM students WHERE total >= 300\")\n",
    "high_score.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "048734b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession 종료\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"✅ SparkSession 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "797c032c-0698-4ddc-82bb-303ccdc91bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PySpark 데이터 분석 + 머신러닝(MLlib) 실습 예제\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SparkSession 생성\n",
    "# ------------------------------------------------------------\n",
    "# - SparkSession은 PySpark의 메인 진입점\n",
    "# - master(\"local[*]\") → CPU 코어 전체 사용\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark_ML_Example\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ SparkSession 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fadb2439-82e6-4cf5-b20e-03f32feabac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV 파일 로드 완료\n",
      "+-----+----+---+---+----+-------+\n",
      "|class|name|kor|eng|math|science|\n",
      "+-----+----+---+---+----+-------+\n",
      "|    1| aaa| 67| 87|  90|     98|\n",
      "|    1| bbb| 45| 45|  56|     98|\n",
      "|    1| ccc| 95| 59|  96|     88|\n",
      "|    1| ddd| 65| 94|  89|     98|\n",
      "|    1| eee| 45| 65|  78|     98|\n",
      "|    1| fff| 78| 76|  98|     89|\n",
      "|    2| ggg| 87| 67|  65|     56|\n",
      "|    2| hhh| 89| 98|  78|     78|\n",
      "|    2| iii|100| 78|  56|     65|\n",
      "|    2| jjj| 99| 89|  87|     87|\n",
      "|    2| kkk| 98| 45|  56|     54|\n",
      "|    2| lll| 65| 89|  87|     78|\n",
      "+-----+----+---+---+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2️⃣ CSV 파일 로드\n",
    "# ------------------------------------------------------------\n",
    "# - header=True : 첫 번째 행을 컬럼명으로 사용\n",
    "# - inferSchema=True : 데이터 타입 자동 추론\n",
    "\n",
    "df = spark.read.csv(\"Subjects.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"✅ CSV 파일 로드 완료\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c1a42fa-f138-42d4-9b27-838977eeb2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터 전처리 완료\n",
      "+-----+----+---+---+----+-------+-----+----+\n",
      "|class|name|kor|eng|math|science|total|pass|\n",
      "+-----+----+---+---+----+-------+-----+----+\n",
      "|    1| aaa| 67| 87|  90|     98|  342|   1|\n",
      "|    1| bbb| 45| 45|  56|     98|  244|   0|\n",
      "|    1| ccc| 95| 59|  96|     88|  338|   1|\n",
      "|    1| ddd| 65| 94|  89|     98|  346|   1|\n",
      "|    1| eee| 45| 65|  78|     98|  286|   0|\n",
      "|    1| fff| 78| 76|  98|     89|  341|   1|\n",
      "|    2| ggg| 87| 67|  65|     56|  275|   0|\n",
      "|    2| hhh| 89| 98|  78|     78|  343|   1|\n",
      "|    2| iii|100| 78|  56|     65|  299|   0|\n",
      "|    2| jjj| 99| 89|  87|     87|  362|   1|\n",
      "|    2| kkk| 98| 45|  56|     54|  253|   0|\n",
      "|    2| lll| 65| 89|  87|     78|  319|   1|\n",
      "+-----+----+---+---+----+-------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 데이터 전처리 및 파생변수 생성\n",
    "# ------------------------------------------------------------\n",
    "# - 결측값은 0으로 대체\n",
    "# - 총점(total) 및 합격여부(pass) 컬럼 생성\n",
    "\n",
    "df = df.fillna({\"kor\": 0, \"eng\": 0, \"math\": 0, \"science\": 0})\n",
    "df = df.withColumn(\"total\", col(\"kor\") + col(\"eng\") + col(\"math\") + col(\"science\"))\n",
    "df = df.withColumn(\"pass\", when(col(\"total\") >= 300, 1).otherwise(0))\n",
    "\n",
    "print(\"✅ 데이터 전처리 완료\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba8f3cc6-09fb-4951-99e4-ea2df511b24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Linear Regression 모델 학습 완료\n",
      "+--------------------+-----+------------------+\n",
      "|            features|total|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[67.0,87.0,90.0,9...|  342| 342.0000000000004|\n",
      "|[45.0,45.0,56.0,9...|  244|243.99999999999918|\n",
      "|[95.0,59.0,96.0,8...|  338|338.00000000000153|\n",
      "|[65.0,94.0,89.0,9...|  346| 346.0000000000002|\n",
      "|[45.0,65.0,78.0,9...|  286|285.99999999999903|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4️⃣ 회귀모델 (Linear Regression)\n",
    "# ------------------------------------------------------------\n",
    "# - 입력 피처: kor, eng, math, science\n",
    "# - 타깃: total\n",
    "# - 목적: 총점(total)을 예측하는 회귀모델\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"kor\", \"eng\", \"math\", \"science\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "'''\n",
    "Spark ML은 feature를 벡터 형태로 받아야 학습 가능\n",
    "여러 컬럼(feature1, feature2...)도 한 컬럼(features)에 합침\n",
    "'''\n",
    "train_df = assembler.transform(df).select(\"features\", \"total\") # x, y set\n",
    "\n",
    "\n",
    "'''featuresCol: 독립 변수 벡터 컬럼\n",
    "\n",
    "labelCol: 예측하려는 종속 변수\n",
    "\n",
    ".fit(df) → 회귀 계수(weight)와 절편(intercept) 학습'''\n",
    "\n",
    "# 회귀 모델 생성 및 학습\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total\")\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# 예측 수행\n",
    "''' .transform(df) → 학습한 모델로 df의 label 예측 '''\n",
    "lr_predictions = lr_model.transform(train_df)\n",
    "\n",
    "print(\"Linear Regression 모델 학습 완료\")\n",
    "lr_predictions.select(\"features\", \"total\", \"prediction\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a850d0ad-143a-4565-adf1-726ec0a8a12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Logistic Regression 모델 학습 완료\n",
      "+---------------------+----+----------+------------------------------------------+\n",
      "|features             |pass|prediction|probability                               |\n",
      "+---------------------+----+----------+------------------------------------------+\n",
      "|[67.0,87.0,90.0,98.0]|1   |1.0       |[2.81944823643108E-9,0.9999999971805518]  |\n",
      "|[45.0,45.0,56.0,98.0]|0   |0.0       |[1.0,0.0]                                 |\n",
      "|[95.0,59.0,96.0,88.0]|1   |1.0       |[1.245296963688586E-8,0.9999999875470303] |\n",
      "|[65.0,94.0,89.0,98.0]|1   |1.0       |[1.852718822498173E-10,0.9999999998147281]|\n",
      "|[45.0,65.0,78.0,98.0]|0   |0.0       |[0.9999999769882363,2.3011763716773714E-8]|\n",
      "+---------------------+----+----------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 5️⃣ 분류모델 (Logistic Regression)\n",
    "# ------------------------------------------------------------\n",
    "# - 입력 피처 동일\n",
    "# - 타깃: pass (합격 여부)\n",
    "# - 목적: 합격(1)/불합격(0) 분류\n",
    "\n",
    "assembler2 = VectorAssembler(\n",
    "    inputCols=[\"kor\", \"eng\", \"math\", \"science\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "train_df2 = assembler2.transform(df).select(\"features\", \"pass\") # x, y set\n",
    "\n",
    "# 분류 모델 생성 및 학습\n",
    "logr = LogisticRegression(featuresCol=\"features\", labelCol=\"pass\")\n",
    "logr_model = logr.fit(train_df2)\n",
    "\n",
    "# 예측 수행\n",
    "logr_predictions = logr_model.transform(train_df2)\n",
    "\n",
    "print(\"✅ Logistic Regression 모델 학습 완료\")\n",
    "logr_predictions.select(\"features\", \"pass\", \"prediction\", \"probability\").show(5, truncate=False)\n",
    "\n",
    "# # ------------------------------------------------------------\n",
    "# # 6️⃣ 예측 결과 MongoDB에 저장\n",
    "# # ------------------------------------------------------------\n",
    "# # - 두 모델 결과를 하나로 병합하여 MongoDB에 저장 가능\n",
    "# # - 여기서는 회귀 결과를 예시로 저장\n",
    "\n",
    "# lr_predictions.write.format(\"mongodb\").mode(\"overwrite\").save()\n",
    "# print(\"✅ 예측 결과 MongoDB 저장 완료\")\n",
    "\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f0a67d9-3396-489b-b8a0-74f1fe14825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession 종료\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6️⃣ SparkSession 종료\n",
    "# ------------------------------------------------------------\n",
    "spark.stop()\n",
    "print(\"✅ SparkSession 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6082be12-e24f-4e40-93a6-50032048595d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ea5ae32",
   "metadata": {},
   "source": [
    "# MongoDB 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f59736f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession 생성 완료\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PySpark ↔ MongoDB 연동 실습 전체 코드\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDB_Spark_Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\", \"C:/spark_jars/mongo-spark-connector_2.12-10.2.0.jar\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://192.168.99.2:27017/testdb.students\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://192.168.99.2:27017/testdb.students_result\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db58e645",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o44.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 2️⃣ MongoDB → DataFrame 로드\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmongodb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muri\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmongodb://localhost:27017/testdb.students\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ MongoDB 데이터 로드 완료\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m df.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda3\\envs\\ai\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:314\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28mself\u001b[39m._jreader.load(\u001b[38;5;28mself\u001b[39m._spark._sc._jvm.PythonUtils.toSeq(path)))\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda3\\envs\\ai\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda3\\envs\\ai\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\anaconda3\\envs\\ai\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o44.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# MongoDB → DataFrame 로드\n",
    "# ------------------------------------------------------------\n",
    "df = spark.read \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .option(\"uri\", \"mongodb://localhost:27017/testdb.students\") \\\n",
    "    .load()\n",
    "\n",
    "print(\" MongoDB 데이터 로드 완료\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448d2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06ab31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89c34d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession 생성 완료\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o883.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21840\\1723034216.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# 2️⃣ MongoDB → DataFrame 로드\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mongodb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"✅ MongoDB 데이터 로드 완료\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     def json(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o883.load.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: mongodb. Please find packages at `https://spark.apache.org/third-party-projects.html`.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:724)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:208)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.lang.ClassNotFoundException: mongodb.DefaultSource\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\r\n\tat scala.util.Failure.orElse(Try.scala:224)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#  PySpark ↔ MongoDB 연동 실습 전체 코드\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "#  SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDB_Spark_Test\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars\", \"C:/spark_jars/mongo-spark-connector_2.12-10.2.0.jar\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://192.168.99.2:27017/testdb.students\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://192.168.99.2:27017/testdb.students_result\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\" SparkSession 생성 완료\")\n",
    "\n",
    "#  MongoDB → DataFrame 로드\n",
    "df = spark.read.format(\"mongodb\").load()\n",
    "print(\" MongoDB 데이터 로드 완료\")\n",
    "df.show()\n",
    "\n",
    "#  데이터 처리\n",
    "df = df.withColumn(\"total\", col(\"kor\") + col(\"eng\") + col(\"math\") + col(\"science\"))\n",
    "df = df.withColumn(\"pass\", when(col(\"total\") >= 300, 1).otherwise(0))\n",
    "print(\" 데이터 처리 완료\")\n",
    "df.show()\n",
    "\n",
    "#  결과 MongoDB로 저장\n",
    "df.write \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "print(\" MongoDB 저장 완료\")\n",
    "\n",
    "#  SparkSession 종료\n",
    "spark.stop()\n",
    "print(\" SparkSession 종료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e80d05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
